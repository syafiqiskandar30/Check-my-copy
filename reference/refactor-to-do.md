üöß Setel UX Writing Assistant ‚Äî Backend Refactor Instructions (For Codex)

Goal: Fix backend behavior so the model consistently follows our UX writing style guide

You are editing a TypeScript codebase for a Figma-style UX writing assistant for Setel.
The main backend logic lives in src/code.ts.

This assistant currently:
	‚Ä¢	Injects the Setel UX writer persona into the system prompt
	‚Ä¢	Parses our large JSON style guide
	‚Ä¢	Enforces tones, banned terms, and length presets
	‚Ä¢	Validates outputs with validateVariant
	‚Ä¢	Rebuilds prompts with no memory drift

However, several backend elements are incomplete or missing, causing inconsistency.

Your task is to update src/code.ts to implement the fixes below.

‚∏ª

‚úÖ 1. Add explicit generation parameters (temperature, top_p, max tokens)

Problem
	‚Ä¢	The Gemini API call does NOT set temperature or top_p.
	‚Ä¢	We rely on unknown defaults.
	‚Ä¢	No max token guard.

Required Changes
	1.	Locate all areas where the model API is called (around src/code.ts:1362‚Äì1407).
	2.	Add explicit config: 
        temperature: 0.3,
        top_p: 0.9,
        maxOutputTokens: 512

Use correct field names depending on the API (generationConfig, samplingParams, etc.).

Acceptance Criteria
	‚Ä¢	Every rewrite call uses temperature=0.3, top_p=0.9, maxOutputTokens=512.

‚∏ª

‚úÖ 2. Add a structured TASK_SPEC header

Problem

Tone, length, and context are only present in narrative text.
The model needs a structured, machine-readable representation.

Required Changes

Inside buildRewriteInstructions (lines ~1114‚Äì1235):
	1.	Before natural-language instructions, insert a block:

TASK_SPEC:
        {
            "task": "rewrite",
             "tone": "<TONE_KEY>",
             "length": "<LENGTH_KEY>",
             "context": "<CONTEXT_KEY>"
        }
END_TASK_SPEC  

	<TONE_KEY> = exact key from tone_palette.available_tones
	‚Ä¢	<LENGTH_KEY> = "short" | "medium" | "long"
	‚Ä¢	<CONTEXT_KEY> = "button" | "toast" | "modal" | "general_ui_copy" (fallback)

	2.	Add:
‚ÄúAlways follow TASK_SPEC exactly when choosing tone, length, and context.‚Äù

Acceptance Criteria
	‚Ä¢	Every prompt contains a TASK_SPEC block.
	‚Ä¢	Instructions explicitly reference TASK_SPEC.

‚∏ª

‚úÖ 3. Require JSON-only output (structured variants)

Problem
	‚Ä¢	The model currently returns plain text blocks.
	‚Ä¢	Should instead return JSON so we can reliably parse the output.

Required Changes
	1.	Update instructions in buildRewriteInstructions:

Tell the model:

Respond ONLY with valid JSON in this format:

{
  "variants": [
    {
      "tone": "<TONE_KEY>",
      "length": "<LENGTH_KEY>",
      "text": "..."
    },
    {
      "tone": "<TONE_KEY>",
      "length": "<LENGTH_KEY>",
      "text": "..."
    }
  ]
}

2.	Update the response handler (lines ~1362‚Äì1407):
	‚Ä¢	Parse JSON safely.
	‚Ä¢	If parsing fails:
	‚Ä¢	attempt to extract the JSON substring OR
	‚Ä¢	fallback to treat the entire text as a single variant.
	3.	Pass each variant.text through validateVariant.
	4.	Remove variants.join("\n\n") and rely on parsed JSON.

Acceptance Criteria
	‚Ä¢	Model returns JSON only.
	‚Ä¢	Code parses the JSON and validates each variant.
	‚Ä¢	Final output is consistent and structured.

‚∏ª

‚úÖ 4. Add fallback logic for invalid tone inputs

Problem
	‚Ä¢	If UI sends an invalid tone name, backend doesn‚Äôt detect or correct it.

Required Changes

In tone resolution (collectToneConfigs in lines 346‚Äì475):
	1.	Add:

if (!tone_palette.available_tones[requestedTone]) {
    requestedTone = "neutral_helpful"; // safe fallback
}

	2.	Optionally log a warning (non-blocking).

Acceptance Criteria
	‚Ä¢	Invalid tone ‚Üí fallback to neutral_helpful (or friendly).
	‚Ä¢	Backend never sends unknown tone keys to the model.

‚∏ª

‚úÖ 5. Add prompt sanitisation

Problem
	‚Ä¢	Input is not cleaned.
	‚Ä¢	No normalization of whitespace or removal of stray characters.

Required Changes

Create a utility function:

function sanitisePromptText(input: string): string {
  return input
    .replace(/\s+/g, " ")      // collapse whitespace
    .replace(/[\u0000-\u001F]/g, "") // remove control chars
    .trim();
}

Before building prompt:
	‚Ä¢	Run user input through sanitisePromptText
	‚Ä¢	Apply same sanitisation to any embedded examples if needed

Acceptance Criteria
	‚Ä¢	Sanitized user text always used in prompt creation.
	‚Ä¢	No weird spacing or control characters sent to model.

‚∏ª

‚úÖ 6. Add prompt length guard

Problem
	‚Ä¢	No token or length measurement.
	‚Ä¢	Potential for runaway prompt sizes.

Required Changes

Before sending final prompt:

const approxTokens = Math.floor(prompt.length / 4);
if (approxTokens > 6000) {
    // Trim lowest priority sections (extra examples)
    // Keep core persona, TASK_SPEC, tone rules, banned terms
}

Acceptance Criteria
	‚Ä¢	Prompt never grows beyond a safe threshold.
	‚Ä¢	Critical rules are never dropped.

‚∏ª

‚ö†Ô∏è 7. Maintain existing functionality

Do not break:
	‚Ä¢	validateVariant
	‚Ä¢	collectGuideBannedTerms
	‚Ä¢	Setel persona injection
	‚Ä¢	Banned terms checks
	‚Ä¢	Tone + length enforcement
	‚Ä¢	Fresh prompt generation for each rewrite

‚∏ª

üéØ Final Goal

After implementing these changes:
	‚Ä¢	Prompts are structured, stable, and machine-readable.
	‚Ä¢	Model obeys tone and length with far higher consistency.
	‚Ä¢	Outputs come back as clean JSON variants.
	‚Ä¢	Randomness is controlled via temperature + top_p.
	‚Ä¢	Invalid tones fall back safely.
	‚Ä¢	Prompt sanitization and size guards prevent unexpected behavior.

This will significantly improve the reliability of the UX writing assistant even before compacting the style guide.
